{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Multilayer Perceptron implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import util\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# True if you don't want to use the test data and you just want to evaluate the model\n",
    "# on the training data\n",
    "testing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('X_train.pkl') as f:\n",
    "    X_train = pickle.load(f).todense()\n",
    "with open('t_train.pkl') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "with open('X_test.pkl') as f:\n",
    "    X_test = pickle.load(f).todense()\n",
    "with open('test_ids.pkl') as f:\n",
    "    test_ids = pickle.load(f)\n",
    "    \n",
    "nclass = 1 + Y_train.max()\n",
    "Y_one_hot_train = np.eye(nclass)[Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    train_data, test_data, train_label, test_label = train_test_split(X_train, Y_one_hot_train, test_size=0.20, random_state=42)  \n",
    "else:\n",
    "    train_data = X_train\n",
    "    test_data = X_test\n",
    "    train_label=Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = train_data.shape[0]\n",
    "dim = train_data.shape[1]\n",
    "ntest = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468 train images loaded\n",
      "618 test images loaded\n",
      "31 dimensional input\n",
      "15 classes\n"
     ]
    }
   ],
   "source": [
    "print (\"%d train images loaded\" % (ntrain))\n",
    "print (\"%d test images loaded\" % (ntest))\n",
    "print (\"%d dimensional input\" % (dim))\n",
    "print (\"%d classes\" % (nclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready to Go!\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(0)\n",
    "# Parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 9000\n",
    "batch_size      = ntrain\n",
    "display_step    = 1000\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 20 # 1st layer num features\n",
    "n_hidden_2 = 20 # 2nd layer num features\n",
    "n_input    = dim # data input \n",
    "n_classes  = nclass # total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2']))\n",
    "    return tf.matmul(layer_2, _weights['out']) + _biases['out']\n",
    "    \n",
    "# Store layers weight & bias\n",
    "stddev = 0.1 # <== This greatly affects accuracy!! \n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "print (\"Network Ready to Go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "final_pred = tf.argmax(pred,1)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) \n",
    "optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "print (\"Functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# OPTIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/9000 cost: 21.363397598\n",
      " Training accuracy: 0.094\n",
      " Test accuracy: 0.108\n",
      "Epoch: 1000/9000 cost: 1.334604979\n",
      " Training accuracy: 0.630\n",
      " Test accuracy: 0.618\n",
      "Epoch: 2000/9000 cost: 1.224472761\n",
      " Training accuracy: 0.669\n",
      " Test accuracy: 0.670\n",
      "Epoch: 3000/9000 cost: 1.129404664\n",
      " Training accuracy: 0.685\n",
      " Test accuracy: 0.673\n",
      "Epoch: 4000/9000 cost: 1.091687441\n",
      " Training accuracy: 0.677\n",
      " Test accuracy: 0.657\n",
      "Epoch: 5000/9000 cost: 1.072608352\n",
      " Training accuracy: 0.675\n",
      " Test accuracy: 0.670\n",
      "Epoch: 6000/9000 cost: 1.044591784\n",
      " Training accuracy: 0.699\n",
      " Test accuracy: 0.678\n",
      "Epoch: 7000/9000 cost: 1.010546803\n",
      " Training accuracy: 0.703\n",
      " Test accuracy: 0.688\n",
      "Epoch: 8000/9000 cost: 1.041078091\n",
      " Training accuracy: 0.689\n",
      " Test accuracy: 0.670\n",
      "Optimization Finished!\n",
      "CPU times: user 4min 1s, sys: 13.8 s, total: 4min 15s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(ntrain/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        randidx = np.random.randint(ntrain, size=batch_size)\n",
    "        batch_xs = train_data[randidx, :]\n",
    "        batch_ys = train_label[randidx, :]   \n",
    "        # Fit training using batch data\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, \n",
    "                feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % \n",
    "               (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        if testing:\n",
    "            test_acc = sess.run(accr, feed_dict={x: test_data, y: test_label})\n",
    "            print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "    if epoch == training_epochs - 1:       \n",
    "        y_pred = sess.run(final_pred, feed_dict={x: test_data})\n",
    "        \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    util.write_predictions(y_pred, test_ids, 'predictions_9k_nn_orig_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
